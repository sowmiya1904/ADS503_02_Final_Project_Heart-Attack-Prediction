---
title: "Heart Attack Prediction"
output: html_document
date: "2024-06-12"
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

## Exploratory Data Analysis

```{r}
# Reading contents into a data frame
data_df <- read.csv("heart.csv", header = TRUE)
head(data_df)

# Dimension of data frame
dim(data_df)

# Structure of data frame
str(data_df)

# missing values
sum(is.na(data_df))
```

### Correlation Matrix Plot

```{r}
library(corrplot)
corrplot(cor(data_df), method = 'number')
```

The correlation plot indicates that the relationships between the predictors are moderate to weak, as evidenced by correlation coefficients below 0.55. This suggests that the predictors are not strongly correlated with one another.

### Distribution of Categorical Variables

```{r}
# Converting categorical features to factors

data_df$sex <- factor(data_df$sex, levels = c(0, 1))
data_df$cp <- factor(data_df$cp, levels = c(0, 1, 2, 3))
data_df$fbs <- factor(data_df$fbs, levels = c(0, 1))
data_df$restecg <- factor(data_df$restecg, levels = c(0, 1, 2))
data_df$exang <- factor(data_df$exang, levels = c(0, 1))
data_df$slope <- factor(data_df$slope, levels = c(0, 1, 2))
data_df$thal <- factor(data_df$thal, levels = c(0, 1, 2, 3))
data_df$target <- factor(data_df$target, levels = c(0, 1), labels = c("No", "Yes"))

str(data_df)
```

### Descriptive Statistics

```{r}
summary(data_df)
```

### Histogram distribution of numerical predictors

```{r}
# subset numeric predictors
data_df_numeric <- data_df[, sapply(data_df, is.numeric)]

library(Hmisc)
hist.data.frame(data_df_numeric)
```

The plot above reveals that numerical predictors display distributions that does not follow a normal distribution. 

### Distribution of categorical features

```{r}
library(ggplot2)

factor_cols_df <- data_df[sapply(data_df, is.factor)]
columns <- colnames(factor_cols_df)

# Function to plot the distribution of categorical features
lapply(columns, function(col) {
  ggplot(factor_cols_df, aes_string(x = col)) + geom_bar() + 
    labs(title = paste("Distribution of", col), x = col, y = "Count")
})
```

### Boxplot distribution to detect presence of outliers

```{r}
par(mfrow = c(2, 3))
for (col in 1:(ncol(data_df_numeric))) {
  boxplot(data_df_numeric[,col], main = colnames(data_df_numeric)[col], xlab = colnames(data_df_numeric)[col])
}
```

The above boxplot indicates the presence of outliers across several predictors including trestbps, chol, thalach, oldpeak and ca. Notably only one predictor, age, appears to be devoid of outliers.

## Data Preprocessing

```{r boxcox}
library(caret)

# Applying BoxCox transformation to handle skewness
bct_trans <- preProcess(data_df[, numeric_cols], method = "BoxCox")
trans_data <- predict(bct_trans, data_df[, numeric_cols])
hist(trans_data)

data_df[, numeric_cols] <- trans_data
```

```{r}
# Distribution of the classes
table(data_df$target)
```

The above output shows that the classes are distributed equally and there is no class imbalance problem in the given dataset.

## Data Splitting

```{r data splitting}
set.seed(476)

# Split the data into training and test sets
index <- createDataPartition(data_df$target, p = .8, list = FALSE)

trainData <- data_df[index,]
testData <- data_df[-index,]

# Separate predictors and target variable
xTrain <- subset(trainData, select = -c(target))
yTrain <- trainData$target
xTest <- subset(testData, select = -c(target))
yTest <- testData$target
```

## Exploratory Data Analysis (EDA)

```{r}
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(caret)
library(corrplot)

# Load the dataset
data <- read.csv('heart.csv')

# Display the structure and summary of the dataset
str(data)
summary(data)

# Check for missing values
colSums(is.na(data))

# Correlation matrix
correlation_matrix <- cor(data)
corrplot(correlation_matrix, method = "circle")

# Visualize the distribution of target variable
ggplot(data, aes(x = factor(target))) + 
  geom_bar(fill = 'blue') +
  labs(title = "Distribution of Target Variable", x = "Target", y = "Count")

# Visualize the distribution of age
ggplot(data, aes(x = age)) + 
  geom_histogram(fill = 'green', bins = 30) +
  labs(title = "Distribution of Age", x = "Age", y = "Count")

# Boxplot of age vs target
ggplot(data, aes(x = factor(target), y = age)) + 
  geom_boxplot(fill = 'orange') +
  labs(title = "Boxplot of Age vs Target", x = "Target", y = "Age")

# Barplot of sex vs target
ggplot(data, aes(x = factor(sex), fill = factor(target))) + 
  geom_bar(position = "dodge") +
  labs(title = "Barplot of Sex vs Target", x = "Sex", fill = "Target")
```

### Modeling

```{r}
# Split the data into training and testing sets
set.seed(42)
trainIndex <- createDataPartition(data$target, p = .7, 
                                  list = FALSE, 
                                  times = 1)
dataTrain <- data[ trainIndex,]
dataTest  <- data[-trainIndex,]

# Train a logistic regression model
log_model <- train(target ~ ., data = dataTrain, method = "glm", family = "binomial")

# Train a random forest model
rf_model <- train(target ~ ., data = dataTrain, method = "rf")

# Train an XGBoost model
xgb_model <- train(target ~ ., data = dataTrain, method = "xgbTree")

# Evaluate models
log_pred <- predict(log_model, newdata = dataTest)
rf_pred <- predict(rf_model, newdata = dataTest)
xgb_pred <- predict(xgb_model, newdata = dataTest)

log_conf_matrix <- confusionMatrix(log_pred, dataTest$target)
rf_conf_matrix <- confusionMatrix(rf_pred, dataTest$target)
xgb_conf_matrix <- confusionMatrix(xgb_pred, dataTest$target)

# Print results
print(log_conf_matrix)
print(rf_conf_matrix)
print(xgb_conf_matrix)

# Ensembling using stacking
model_list <- caretList(
  target ~ ., data = dataTrain,
  trControl = trainControl(method = "cv", number = 10),
  methodList = c("glm", "rf", "xgbTree")
)

stack_control <- trainControl(method = "cv", number = 10)
stack.glm <- caretStack(model_list, method = "glm", trControl = stack_control)

# Predict on test data
stack_pred <- predict(stack.glm, newdata = dataTest)

# Confusion matrix and accuracy
stack_conf_matrix <- confusionMatrix(stack_pred, dataTest$target)
stack_acc_score <- stack_conf_matrix$overall['Accuracy'] * 100

# Print results
print(stack_conf_matrix)
print(paste("Accuracy of StackingCVClassifier:", stack_acc_score))
```

### Logistic Regression

```{r}
# Set training control for model building
ctrl <- trainControl(method = "cv", summaryFunction = twoClassSummary,
                     classProbs = TRUE, savePredictions = TRUE)

set.seed(100)

logitFit <- train(x = xTrain, y = yTrain, method = "glm", metric = "ROC",
                  trControl = ctrl, preProc = c("center", "scale"))

logitFit

# Make predictions
logitPred <- predict(logitFit, xTest)

# Confusion Matrix
logitCM <- confusionMatrix(logitPred, yTest, positive = "Yes")
logitCM
```

