---
title: "Heart Attack Prediction"
output: html_document
date: "2024-06-12"
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library(ggplot2)
library(dplyr)
library(caret)
library(corrplot)
library(Hmisc)
library(pROC)
```

## Exploratory Data Analysis

```{r}
# Reading contents into a data frame
data_df <- read.csv("heart.csv", header = TRUE)
head(data_df)

# Dimension of data frame
dim(data_df)

# Structure of data frame
str(data_df)

# missing values
sum(is.na(data_df))
```

```{r}
# Converting categorical features to factors

data_df$sex <- factor(data_df$sex, levels = c(0, 1))
data_df$cp <- factor(data_df$cp, levels = c(0, 1, 2, 3))
data_df$fbs <- factor(data_df$fbs, levels = c(0, 1))
data_df$restecg <- factor(data_df$restecg, levels = c(0, 1, 2))
data_df$exang <- factor(data_df$exang, levels = c(0, 1))
data_df$slope <- factor(data_df$slope, levels = c(0, 1, 2))

# Update "thal" feature categories
data_df$thal <- as.character(data_df$thal)
data_df$thal[data_df$thal == '0'] = "Normal"
data_df$thal[data_df$thal == '1'] = "Normal"
data_df$thal[data_df$thal == '2'] = "Fixed Defect"
data_df$thal[data_df$thal == '3'] = "Reversable Defect"
data_df$thal = as.factor(data_df$thal)

data_df$target <- factor(data_df$target, levels = c(1, 0), labels = c("Yes", "No"))

# structure of data frame
str(data_df)
```

```{r}
# Descriptive Statistics
summary(data_df)
```

### Histogram distribution of numerical predictors

```{r}
# subset numeric predictors
data_df_numeric <- data_df[, sapply(data_df, is.numeric)]

# Distribution of numerical predictors
hist.data.frame(data_df_numeric)
```

The plot above reveals that numerical predictors display distributions that does not follow a normal distribution. 

### Distribution of categorical features

```{r}
# Barplot of sex vs target
ggplot(data_df, aes(x = sex, fill = target)) + 
  geom_bar(position = "dodge") +
  labs(title = "Barplot of Sex vs Target", x = "Sex", fill = "Target", 
       caption = "0 = Female, 1 = Male")
```

```{r}
# Barplot of chest pain (cp) vs target
ggplot(data_df, aes(x = cp, fill = target)) + 
  geom_bar(position = "dodge") +
  labs(title = "Barplot of Chest Pain (cp) vs Target", x = "Chest Pain", fill = "Target", 
  caption = "0 = Typical Angina, 1 = Atypical Angina, 3 = Non-anginal Pain, 4 = Asymptomatic")
```

```{r}
# Barplot of Fasting Blood Sugar (fbs) vs target
ggplot(data_df, aes(x = fbs, fill = target)) + 
  geom_bar(position = "dodge") +
  labs(title = "Barplot of Fasting Blood Sugar (fbs) vs Target", x = "FBS", fill = "Target", 
       caption = "0 = False, 1 = True")
```

```{r}
# Barplot of Resting Electrocardiographic Results (restecg) vs target
ggplot(data_df, aes(x = restecg, fill = target)) + 
  geom_bar(position = "dodge") +
  labs(title = "Barplot of Resting Electrocardiographic Results (restecg) vs target", 
       x = "RestECG", fill = "Target", 
       caption = "0 = Normal, 1 = ST-T wave Abnormality, 2 = Left Ventricular Hypertrophy")
```

```{r}
# Barplot of Exercise Induced Angina (exang) vs target
ggplot(data_df, aes(x = exang, fill = target)) + 
  geom_bar(position = "dodge") +
  labs(title = "Barplot of Exercise Induced Angina (exang) vs target", x = "Exang", fill = "Target",
       caption = "0 = No, 1 = Yes")
```

```{r}
# Barplot of Slope vs target
ggplot(data_df, aes(x = slope, fill = target)) + 
  geom_bar(position = "dodge") +
  labs(title = "Barplot of Slope vs target", x = "Slope", fill = "Target",
       caption = "0 = Upsloping, 1 = Flat, 2 = Downsloping")
```

```{r}
# Barplot of thal vs target
ggplot(data_df, aes(x = thal, fill = target)) + 
  geom_bar(position = "dodge") +
  labs(title = "Barplot of Thal vs target", x = "Thal", fill = "Target")
```

### Boxplot distribution to detect presence of outliers

```{r}
par(mfrow = c(2, 3))
for (col in 1:(ncol(data_df_numeric))) {
  boxplot(data_df_numeric[,col], main = colnames(data_df_numeric)[col], xlab = colnames(data_df_numeric)[col])
}
```

The above boxplot indicates the presence of outliers across several predictors including trestbps, chol, thalach, oldpeak and ca. Notably only one predictor, age, appears to be devoid of outliers.

```{r}
# Correlation matrix plot between numerical features
corrplot(cor(data_df[c('age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'ca')]), method = 'number')
```

The correlation plot indicates that a moderate relationship between the predictors, as evidenced by correlation coefficients below 0.55. This suggests that the predictors are not strongly correlated with one another.

## Data Preprocessing

### Feature Selection

**We implement RFE (Recursive Feature Elimination) to identify optimal features in the dataset to predict the diagnosis of heart disease.**

```{r}
# Define the control using random forest function
rfeCtrl <- rfeControl(functions = rfFuncs, method = "repeatedcv", repeats = 5, number = 10)

# Separate predictors and target variable
x <- subset(data_df, select = -c(target))
y <- data_df$target

# Data Partioning
set.seed(476)
index <- createDataPartition(y, p = 0.8, list = FALSE)

# Training and Test sets
xTrain <- x[ index, ]
xTest <- x[-index, ]
yTrain <- y[ index]
yTest <- y[-index]

```


```{r}
# Train RFE model
rfeTune <- rfe(xTrain, yTrain, sizes = c(1:13), rfeControl = rfeCtrl)
rfeTune
```

```{r}
# Extract the selected features
selected_vars <- predictors(rfeTune)
selected_vars

# Extract feature importance 
rfeImp <- varImp(rfeTune, scale = FALSE)

# Convert to a data frame for plotting
importance_df <- data.frame(
  Feature = rownames(rfeImp),
  Importance = rfeImp[ ,1]
)

# Plot feature importance
ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance, fill = Importance)) + 
  geom_bar(stat = "identity") + xlab("Features") + ylab("Importance") +
  ggtitle("Feature Importance Plot using Random Forest") + scale_fill_gradient(low = "blue", high = "red") +
  theme_minimal()
```

## Data Preparation for Modeling

```{r}
# Transfer optimal features and target variable to a new data frame
final_df <- data_df[, c(selected_vars, "target")]

# Encode categorical values (thal - thalassemia status) to numerical values
final_df <- final_df %>% mutate(thal = recode(thal, 
                                              "Normal" = 0,
                                              "Fixed Defect" = 1,
                                              "Reversable Defect" = 2))

# Convert thal to a factor
final_df$thal <- factor(final_df$thal, levels = c(0,1,2))

head(final_df)
```

### Checking for Class Imbalance

```{r}
# Distribution of the classes
table(final_df$target)

# Distribution of chances of heart attack 
ggplot(final_df, aes(x = target, fill = target)) + 
  geom_bar(position = "dodge") +
  labs(title = "Distribution of chances of heart attack", x = "Heart Attack", 
       y = "Count", fill = "Target", caption = "Yes = Presence, No = Absence")
```

Due to the limited size of the dataset with only 303 observations, it is advised against splitting it into separate training and test sets. Instead, resampling methods are utilized for both training the model and assessing its performance. Additionally, the distribution of the response variable shows approximately 54% 'Yes' and 45% 'No', indicating that the dataset is not heavily imbalanced.


```{r}
# Separate predictors and target variable
X_train <- subset(final_df, select = -c(target))
y_target <- final_df$target

# Set training control
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 5, 
                     summaryFunction = twoClassSummary, 
                     classProbs = TRUE, savePredictions = TRUE)
```

# Modeling

## Logistic Regression

```{r}
# Model training
lr_model <- train(x = X_train, y = y_target, method = "glm", metric = "ROC", trControl = ctrl, 
                  preProc = c("center", "scale"))
lr_model
```

```{r}
# Make predictions
lr_preds <- predict(lr_model, X_train)

# Confusion Matrix
lr_CM <- confusionMatrix(lr_preds, y_target)
lr_CM

# Plot the ROC curve
lr_ROC <- roc(lr_model$pred$obs, lr_model$pred$Yes, levels = rev(levels(lr_model$pred$obs)))
plot(lr_ROC, legacy.axes = TRUE)
```

## Mixture Discriminant Analysis

```{r}
set.seed(476)

mda_model <- train(x = X_train, y = y_target, method = "mda", 
                   tuneGrid = expand.grid(subclasses=1:3),
                   metric = "ROC", trControl = ctrl)
mda_model
```

```{r}
# Make predictions
mda_preds <- predict(mda_model, X_train)

# Confusion Matrix
mda_CM <- confusionMatrix(mda_preds, y_target)
mda_CM

# Plot the ROC curve
mda_ROC <- roc(mda_model$pred$obs, mda_model$pred$Yes, levels = rev(levels(mda_model$pred$obs)))
plot(mda_ROC, legacy.axes = TRUE)
```

## Penalized Logistic Regression

```{r}
set.seed(476)

plr_grid <- expand.grid(alpha = c(0, .1, .2, .4, .6, .8, 1),
                        lambda = seq(.01, .2, length = 10))
# Train the model
plr_model <- train(x = X_train, y = y_target, method = "glmnet", tuneGrid = plr_grid, 
                   metric = "ROC", preProc = c("center", "scale"), trControl = ctrl)
plr_model
```

```{r}
# Make predictions
plr_preds <- predict(plr_model, X_train)

# Confusion Matrix
plr_CM <- confusionMatrix(plr_preds, y_target)
plr_CM

# Plot the ROC curve
plr_ROC <- roc(plr_model$pred$obs, plr_model$pred$Yes, levels = rev(levels(plr_model$pred$obs)))
plot(plr_ROC, legacy.axes = TRUE)
```

## Neural Network

```{r}
set.seed(476)

nnet_grid <- expand.grid(size = 1:3, decay = c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 1, 2))

nnet_model <- train(x = X_train, y = y_target, method = "nnet", tuneGrid = nnet_grid,
                    metric = "ROC", trace = FALSE, maxit = 2000, trControl = ctrl,
                    preProc = c("center", "scale"))
nnet_model
```

```{r}
# Make predictions
nnet_preds <- predict(nnet_model, X_train)

# Confusion Matrix
nnet_CM <- confusionMatrix(nnet_preds, y_target)
nnet_CM

# Plot the ROC curve
nnet_ROC <- roc(nnet_model$pred$obs, nnet_model$pred$Yes, levels = rev(levels(nnet_model$pred$obs)))
plot(nnet_ROC, legacy.axes = TRUE)
```

## K-Nearest Neighbors

```{r}
set.seed(476)

knn_model <- train(x = X_train, y = y_target, method = "knn", tuneLength = 20,
                   metric = "ROC", trControl = ctrl, preProc = c("center", "scale"))
knn_model
```

```{r}
# Make predictions
knn_preds <- predict(knn_model, X_train)

# Confusion Matrix
knn_CM <- confusionMatrix(knn_preds, y_target)
knn_CM

# Plot the ROC curve
knn_ROC <- roc(knn_model$pred$obs, knn_model$pred$Yes, levels = rev(levels(knn_model$pred$obs)))
plot(knn_ROC, legacy.axes = TRUE)
```

## Decision Trees

```{r}
set.seed(476)

dt_model <- train(x = X_train, y = y_target, method = "rpart", tuneLength = 30,
                  metric = "ROC", trControl = ctrl, preProc = c("center", "scale"))
dt_model
```

```{r}
# Make predictions
dt_preds <- predict(dt_model, X_train)

# Confusion Matrix
dt_CM <- confusionMatrix(dt_preds, y_target)
dt_CM

# Plot the ROC curve
dt_ROC <- roc(dt_model$pred$obs, dt_model$pred$Yes, levels = rev(levels(dt_model$pred$obs)))
plot(dt_ROC, legacy.axes = TRUE)

```

## Random Forest

```{r}
set.seed(476)

mtry_values <- seq(1, min(10, ncol(X_train)), 1)

rf_model <- train(x = X_train, y = y_target, method = "rf", ntree = 1000,
                  tuneGrid = data.frame(mtry = mtry_values),
                  metric = "ROC", trControl = ctrl, preProc = c("center", "scale"))
rf_model
```

```{r}
# Make predictions
rf_preds <- predict(rf_model, X_train)

# Confusion Matrix
rf_CM <- confusionMatrix(rf_preds, y_target)
rf_CM

# Plot the ROC curve
rf_ROC <- roc(rf_model$pred$obs, rf_model$pred$Yes, levels = rev(levels(rf_model$pred$obs)))
plot(rf_ROC, legacy.axes = TRUE)
```

# Models Evaluation

## Model Performance based on ROC-AUC and Accuracy metrics

```{r}
# ROC-AUC Results
lr_AUC <- auc(lr_ROC)
mda_AUC <- auc(mda_ROC)
plr_AUC <- auc(plr_ROC)
nnet_AUC <- auc(nnet_ROC)
knn_AUC <- auc(knn_ROC)
dt_AUC <- auc(dt_ROC)
rf_AUC <- auc(rf_ROC)

# Accuracy Results
lr_Acc <- lr_CM$overall['Accuracy']
mda_Acc <- mda_CM$overall['Accuracy']
plr_Acc <- plr_CM$overall['Accuracy']
nnet_Acc <- nnet_CM$overall['Accuracy']
knn_Acc <- knn_CM$overall['Accuracy']
dt_Acc <- dt_CM$overall['Accuracy']
rf_Acc <- rf_CM$overall['Accuracy']

metrics_df <- data.frame(
  Model = c("Logistic Regression", "Mixture Discriminant Analysis", "Penalized Logistic Regression",
                                 "Neural Network", "K-Nearest Neighbor", "Decision Trees", "Random Forest"),
  ROC_AUC = c(lr_AUC, mda_AUC, plr_AUC, nnet_AUC, knn_AUC, dt_AUC, rf_AUC),
  Accuracy = c(lr_Acc, mda_Acc, plr_Acc, nnet_Acc, knn_Acc, dt_Acc, rf_Acc)
)

metrics_df
```

## Models ROC_AUC Comparison

```{r}
plot(lr_ROC, type = "s", col = "red", legacy.axes = TRUE)
plot(mda_ROC, type = "s", add = TRUE, col = "green", legacy.axes = TRUE)
plot(plr_ROC, type = "s", add = TRUE, col = "orange", legacy.axes = TRUE)
plot(nnet_ROC, type = "s", add = TRUE, col = "blue", legacy.axes = TRUE)
plot(knn_ROC, type = "s", add = TRUE, col = "purple", legacy.axes = TRUE)
plot(dt_ROC, type = "s", add = TRUE, col = "brown", legacy.axes = TRUE)
plot(rf_ROC, type = "s", add = TRUE, col = "gray", legacy.axes = TRUE)
legend("bottomright", legend = c("Logistic Regression", "Mixture Discriminant Analysis", "Penalized Logistic Regression",
                                 "Neural Network", "K-Nearest Neighbor", "Decision Trees", "Random Forest"),
       col = c("red", "green", "orange", "blue", "purple", "brown", "gray"), lwd = 2)
title(main = "ROC AUC Curves for each model")
```


## Final Model Selection

```{r}
# Best model based on ROC-AUC
best_AUC <- metrics_df[which.max(metrics_df$ROC_AUC), ]
best_AUC
```

```{r}
# Best model based on Accuracy
best_Acc <- metrics_df[which.max(metrics_df$Accuracy), ]
best_Acc
```

The model with the highest accuracy is the Random Forest with an accuracy of 0.9009. The model with the highest ROC AUC is the Logistic Regression with an AUC of 0.9025. The best model should ideally have both a high ROC AUC and high accuracy. 
Based on the results, Random Forest is considered the best model due to its highest accuracy (0.9009901) and a competitive ROC AUC (0.8925095), making it a strong performer in both metrics.

## Feature Importance Plot of the Optimal model (Random Forest)

```{r}
rf_ImpVar <- varImp(rf_model, scale = FALSE)
plot(rf_ImpVar, top = 8, main = "Important Features for Diagnosis of Heart Attack")
```





